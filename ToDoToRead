
# To *Do* List


### On Data

* data scraping
* data formating procedure (mhd->itk->numpy->tensor)
* data division : train/test/validation

### Normalisation

* Maybe no normalisation. Dans ce cas, soit pas de derniere fct d'activation, soit une RelU
* Si normalisation, laquelle ? maxNorm, gaussNorm ? Comment d√©normaliser ? 




### Training loop

* ~~Reconsider the "resume training"~~
* **Number of update G/D**
* epoch dump
* adaptive learning rate
* progress bar

### Testing
* testing on a dataset mais pas avec les loss mais plutot MSE par ex


### Losses 

* add other possible losses
	- adv_loss : BCE, Wassersein
	- recon_loss : L1, L2, both? 

### Models

* add a simple U-net model
* add parameters to Pix2Pix : n_layers, norm or no norm, dropout, activation functions...


### Device
* init_cuda proprement (automatic selection of the device)


### Utils

* automated output filename
* Show infos about a pth
* Show infos about a model
* Show infos about a dataset
* Verbose during training
* Plots adaptable in many situations (avec ou sans PVfree, dataset ou single img, ...)

### Jean Zay
* **Fichier slurm**
* envoyer code&dataset
* generer dataset **SUR** le WORK de jean-zay (pas chez moi)


### General
* setup.py for scripts in a bins folder



# To *Read* List

- A Partial Volume Correction article (PET, MRI, SPECT... cque tu veux mais il faut lire un peu la)

- ideal batchsize

- projection image preprocessing : norm or no norm ? Which norm (max, mean/cov...) ? 


- Inside the NN, norm or no norm ? Which norm (batchNorm2d or instnorm) ?


- optimal use of GPU ? https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19


- training on multi GPU pas forcement compatible avec les batch/inst normalisation. Need of 'syncronised normalisation' is multi-GPU


- Jean Zay et IA : http://www.idris.fr/ia/





