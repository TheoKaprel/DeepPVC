
# To *Do* List


### On Data

* data scraping
* data formating procedure (mhd->itk->numpy->tensor)
* data division : train/test/validation
* data diversity : ellipses, background...

### Normalisation

* Maybe no normalisation. Dans ce cas, soit pas de derniere fct d'activation, soit une RelU
* Si normalisation, laquelle ? maxNorm, gaussNorm ? Comment dÃ©normaliser ? 


###### Max normalisation : 
Advantage : each projection remains in [0,1] then &rarr; easier for training 


Disadvantage : what activation function ? If tanh or sigmoid &rarr; values of PVfree > max(PVE) are unrecoverrable. If Linear &rarr; why not ?


###### Sum normalisation : 

Advantage : the sum of all counts is ~ not depending on the level of Partial Volume Effect







### Training loop

* ~~Reconsider the "resume training"~~
* **Number of update G/D**
* epoch dump
* adaptive learning rate
* progress bar

### Testing
* testing on a dataset mais pas avec les loss mais plutot MSE par ex


### Losses 

* add other possible losses
	- adv_loss : BCE, Wassersein
	- recon_loss : L1, L2, both? 

### Models

* add a simple U-net model
* add parameters to Pix2Pix : n_layers, norm or no norm, dropout, activation functions...


### List of tunable parameters : 
* data_normalisation
* learning rate
* hidden\_channel\_gen
* hidden\_channel\_disc
* generator\_activation
* generator/disc update
* recon\_loss
* lambda\_recon


### Device
* init_cuda proprement (automatic selection of the device)


### Utils

* automated output filename
* Show infos about a model
* Show infos about a dataset
* Verbose during training
* Plots adaptable in many situations (avec ou sans PVfree, dataset ou single img, ...)

### Jean Zay
cava



### General
* setup.py for scripts in a bins folder
* requirement file






# To *Read* List

- A Partial Volume Correction article (PET, MRI, SPECT... cque tu veux mais il faut lire un peu la)

- ideal batchsize

- projection image preprocessing : norm or no norm ? Which norm (max, mean/cov...) ? 


- Inside the NN, norm or no norm ? Which norm (batchNorm2d or instnorm) ?


- optimal use of GPU ? https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19


- training on multi GPU pas forcement compatible avec les batch/inst normalisation. Need of 'syncronised normalisation' is multi-GPU


- Jean Zay et IA : http://www.idris.fr/ia/





